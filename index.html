<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>Bigger is not Always Better: Scaling Properties of Latent Diffusion Models</h1>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2404.01367" class="button">Paper</a>
                <!-- <a href="https://github.com/willisma/SiT" class="button">Code</a> -->
              </div>
            </div>
            <div class="header-image">
                <img src="images/teaser_2.jpg" alt="Teaser Image" class="teaser-image">
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://kfmei.page/" class="author-link">Kangfu Mei</a></p>
                    <p><a href="#" class="author-link">Zhengzhong Tu</a></p>
                    <p><a href="#" class="author-link"> Mauricio Delbracio</a></p>
                    <p><a href="#" class="author-link">Hossein Talebi</a></p>
                    <p><a href="#" class="author-link"> Vishal M. Patel</a></p>
                    <p><a href="#" class="author-link">Peyman Milanfar</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p><a href="#" class="affiliation-link">Johns Hopkins University</a></p>
                    <p><a href="#" class="affiliation-link">Google Research</a></p>
                </div>
                <div class="byline-column">
                    <h3>Date</h3>
                    <p>May. 17, 2024</p>
                </div>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Intro</h4>
                <!-- <div><a href="#introduction">Flow and Diffusion</a></div>
                <div><a href="#framework">Scalable Interpolant Transformers</a></div>
                <div><a href="#conclusion">Conclusion</a></div> -->
            </nav>
        </d-contents>
        
        <p>
            We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency.
            While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size---a critical determinant of sampling efficiency---has not been thoroughly examined.
            Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps.
            Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results.
            Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as comparing performance relative to training compute.
            These findings open up new pathways for the development of LDM scaling strategies which can be employed to enhance generative capabilities within limited inference budgets. 
        </p>
      
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{mei2024bigger,<br>
                &nbsp;&nbsp;title={Bigger is not Always Better: Scaling Properties of Latent Diffusion Models},<br>
                &nbsp;&nbsp;author={Mei, Kangfu and Tu, Zhengzhong and Delbracio, Mauricio and Talebi, Hossein and Patel, Vishal M and Milanfar, Peyman},<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;eprint={2404.01367},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <!-- <script type="text/bibliography">
        
        </script> -->
        <script src="contents_bar.js"></script>
        
        

    </body>

    </body>
</html>
